# RAGFlow 도입 가이드

> 2026-02-16 작성. 회사 환경에 RAG(Retrieval-Augmented Generation) 시스템을 구축하기 위한 조사 및 설정 가이드.

## 목차

- [RAG란?](#rag란)
- [RAGFlow vs Dify 비교](#ragflow-vs-dify-비교)
- [RAGFlow 선택 이유](#ragflow-선택-이유)
- [핵심 개념: 임베딩과 벡터DB](#핵심-개념-임베딩과-벡터db)
- [RAG의 실제 동작 방식](#rag의-실제-동작-방식)
- [용도별 RAG 적합성](#용도별-rag-적합성)
- [LLM 및 임베딩 모델 설정](#llm-및-임베딩-모델-설정)
- [시스템 요구사항](#시스템-요구사항)
- [설치 및 설정 순서](#설치-및-설정-순서)
- [한국어 최적화 팁](#한국어-최적화-팁)

---

## RAG란?

**Retrieval-Augmented Generation** — 문서를 검색(Retrieval)해서 LLM의 답변 생성(Generation)을 보강(Augmented)하는 방식.

LLM은 학습 데이터에 없는 정보(사내 기획서, 시나리오 등)를 모르기 때문에, 관련 문서를 먼저 찾아서 LLM에게 "이거 읽고 대답해"라고 넘기는 구조.

```
사용자 질문
    ↓
벡터DB에서 관련 청크 검색
    ↓
검색된 청크(원문 텍스트) + 질문을 LLM에 전달
    ↓
LLM이 청크 내용 기반으로 답변 생성
```

---

## RAGFlow vs Dify 비교

### 한 줄 요약

- **RAGFlow** = 문서 파싱·검색 특화 RAG 엔진
- **Dify** = RAG 포함 올인원 AI 앱 빌더

### 상세 비교

| 항목 | RAGFlow | Dify |
|---|---|---|
| **주 목적** | 문서 → 지식베이스 → Q&A | AI 워크플로우/앱 빌더 (RAG는 기능 중 하나) |
| **문서 파싱** | ⭐⭐⭐⭐⭐ DeepDoc 엔진. PDF 표·이미지·레이아웃 인식 탁월 | ⭐⭐⭐ 기본적인 파싱. 복잡한 PDF는 약함 |
| **청킹 전략** | 다양 (템플릿별 자동 청킹, 표/QA/요약 등) | 자동/커스텀 청킹, 상대적으로 단순 |
| **검색 품질** | 하이브리드 (벡터 + 키워드 + reranking) 기본 내장 | 벡터 검색 + reranking 지원 |
| **워크플로우** | ❌ 없음. 순수 RAG | ⭐⭐⭐⭐⭐ 비주얼 워크플로우 빌더, 분기·루프·도구호출 |
| **에이전트** | 간단한 대화형 에이전트 | 멀티 에이전트, 도구 연동, 코드 실행 |
| **API** | 검색/대화 API | 앱 API, 워크플로우 API, 임베딩 API 등 풍부 |
| **UI** | 실용적이지만 투박 | 깔끔하고 현대적 |
| **커뮤니티** | GitHub ⭐ ~30K | GitHub ⭐ ~60K+ |
| **LLM 연동** | 외부 API (OpenAI, Claude 등) | 동일 + 로컬 Ollama 등 더 넓은 지원 |
| **최소 RAM** | 8GB 이상 권장 (Elasticsearch 때문) | 4~6GB |
| **라이선스** | Apache 2.0 | Apache 2.0 (일부 Enterprise 기능 제한) |

### 용도별 추천

| 용도 | 추천 |
|---|---|
| 기획서/시나리오 문서 검색 Q&A | **RAGFlow** — 문서 파싱이 핵심 |
| 코드베이스 탐색 + 도구 연동 | **Dify** — 워크플로우로 파이프라인 구성 가능 |
| AI 앱/자동화 전반 | **Dify** — 범용적 |
| 문서 파싱 품질이 최우선 | **RAGFlow** — DeepDoc이 확실히 우위 |

---

## RAGFlow 선택 이유

- 주 용도가 **기획서, 시나리오 등 문서 기반 Q&A**
- DeepDoc 파서가 PDF 표·이미지·레이아웃 처리에서 확실한 우위
- 하이브리드 검색(벡터 + 키워드 + reranking) 기본 제공
- 순수 RAG에 집중하므로 설정이 상대적으로 단순

---

## 핵심 개념: 임베딩과 벡터DB

### 임베딩(Embedding)이란?

텍스트를 고차원 숫자 벡터(좌표)로 변환하는 과정. 의미적으로 비슷한 텍스트끼리 벡터 공간에서 가까이 위치하게 됨.

```
"친구 시스템 초대 제한" → [0.12, -0.34, 0.56, ...]
"초대 수락 최대치"     → [0.11, -0.33, 0.55, ...]  ← 비슷한 벡터!
"PvP 매칭 로직"       → [-0.78, 0.22, -0.41, ...]  ← 먼 벡터
```

### 벡터DB의 저장 구조

흔한 오해: "벡터DB는 벡터만 저장한다" → **아님. 원문 텍스트도 같이 저장함.**

```
문서 → 청크로 분할 → 각 청크를 [벡터 + 원문 텍스트] 쌍으로 저장
```

검색 시:
1. 질문을 벡터로 변환
2. 벡터 유사도로 관련 청크 검색
3. **해당 청크의 원문 텍스트**를 LLM에 전달

---

## RAG의 실제 동작 방식

### 예시 1: 시나리오 대사 검색

질문: "아세라가 울먹이며 대화한 내용이 뭐야?"

```
① 질문 → 벡터 검색
② 청크 히트:
   "에피소드 2-1 / 아세라: (울먹이며) 정말... 이렇게 끝나는 건가요..."
③ LLM이 원문 청크를 읽고 정확한 대사를 답변
```

**청크 안에 원문이 있으므로 원본 파일을 다시 열 필요 없음** — 단, 청크 크기가 충분해야 함.

### 예시 2: 기획서 수치 질문

질문: "친구시스템에서 일일 초대수락 가능한 최대 허용수치가 몇이지?"

```
① "친구시스템 일일 초대수락 최대" → 벡터 검색
② 청크 히트:
   "친구 시스템 기획서 v2.3 / 3.4 초대 제한
    - 일일 초대 발송: 최대 20건
    - 일일 초대 수락: 최대 50건
    - 쿨다운: 수락 후 10분"
③ LLM → "일일 초대수락 최대 허용수치는 50건입니다"
```

### 검색이 실패하는 경우

- 수치가 엑셀 표 안에 있고 파싱이 깨졌을 때
- "초대 관련 수치는 별첨 참고"처럼 다른 문서를 참조할 때
- 청크가 너무 작아서 해당 항목이 잘렸을 때
- 문서 형식이 복잡해서 파서가 구조를 못 잡았을 때

---

## 용도별 RAG 적합성

| 용도 | 적합도 | 비고 |
|---|---|---|
| 시나리오/대사 검색 | ⭐⭐⭐⭐⭐ | 청크 크기만 적절하면 원문까지 바로 나옴 |
| 기획서 수치 질문 | ⭐⭐⭐⭐⭐ | 텍스트 기획서면 거의 완벽 |
| 코드 위치 찾기 | ⭐⭐ | 코드 자체보단 **구조 설명 문서를 벡터화**하는 게 효과적 |
| 코드 내용 파악 | ⭐ | 검색으로 위치 찾고 → 파일 직접 읽는 2단계 방식이 현실적 |

### 소스코드와 RAG

소스코드는 벡터DB가 잘 안 맞는 대표적 케이스:
- 코드는 의미(semantic)보다 **구조(structural)**가 중요
- 변수명 하나 바뀌면 벡터가 완전히 달라짐

**효과적인 접근법:**
- 코드 자체를 벡터화 → ❌ 비효율
- **코드 구조 설명을 벡터화** → ⭐ 유용
  - "PvP 매칭 로직은 `src/pvp/matching/` 아래 `MatchMaker.cs`에 구현"
  - "친구 시스템은 `src/social/friend/` 폴더, 핵심 클래스는 `FriendService`, `InviteHandler`"
- 이런 **아키텍처 맵/인덱스 문서**를 RAG에 넣으면 "어디에 있는지" 빠르게 찾고, 실제 코드는 직접 읽는 방식

---

## LLM 및 임베딩 모델 설정

RAGFlow에는 **두 종류의 모델**이 필요:

### 1. 임베딩 모델 (검색용)

텍스트를 벡터로 변환. 검색 품질을 좌우함.

| 모델 | 한국어 품질 | 비용 | 리소스 | 비고 |
|---|---|---|---|---|
| **BAAI/bge-m3** (내장) | ⭐⭐⭐⭐ | 무료 | RAM 1~2GB | **추천. RAGFlow 기본 지원** |
| intfloat/multilingual-e5-large | ⭐⭐⭐⭐ | 무료 | RAM 1~2GB | 다국어 강자 |
| jhgan/ko-sroberta-multitask | ⭐⭐⭐ | 무료 | 적음 | 한국어 특화, 소규모 |
| OpenAI text-embedding-3-large | ⭐⭐⭐⭐ | 유료 (저렴) | 서버 부담 없음 | RAM 절약 시 |

**권장:** `bge-m3`로 시작. 품질은 OpenAI와 동급이거나 오히려 나은 경우도 있음. 무료.

### 2. LLM (답변 생성용)

검색된 청크를 읽고 자연어 답변을 생성.

| 모델 | 한국어 | 비용 |
|---|---|---|
| **Anthropic Claude** | ⭐⭐⭐⭐⭐ | 기존 API 키 사용 가능 |
| OpenAI GPT-4o | ⭐⭐⭐⭐⭐ | 유료 |
| Qwen2.5 (로컬) | ⭐⭐⭐⭐ | 무료, but RAM 많이 필요 |

**권장:** 기존 Anthropic API 키를 RAGFlow에 설정하면 바로 사용 가능.

> ⚠️ **Anthropic은 임베딩 모델을 제공하지 않음.** Claude는 텍스트 생성만 가능. 임베딩은 반드시 별도 모델 필요.

---

## 시스템 요구사항

### RAGFlow Docker Compose 구성

RAGFlow는 아래 컴포넌트를 Docker로 띄움:

- RAGFlow 서버 (API + UI)
- Elasticsearch (또는 Infinity) — 벡터 + 전문검색
- MySQL/Redis — 메타데이터, 캐시
- 임베딩 모델 서버 (bge-m3 등)

### 최소 사양

| 항목 | 최소 | 권장 |
|---|---|---|
| CPU | 4코어 | 8코어+ |
| RAM | 8GB | 16GB |
| 디스크 | 50GB | 100GB+ (문서량에 따라) |
| GPU | 불필요 (CPU로 가능) | 있으면 임베딩 속도 향상 |

> ⚠️ **Elasticsearch가 메모리를 많이 잡아먹음.** 4GB RAM으로는 부족. 최소 8GB 필요.

---

## 설치 및 설정 순서

### Step 1: Docker 환경 준비

```bash
# Docker & Docker Compose 설치 확인
docker --version
docker compose version
```

### Step 2: RAGFlow 클론 및 실행

```bash
git clone https://github.com/infiniflow/ragflow.git
cd ragflow/docker
docker compose up -d
```

기본 포트: `http://localhost:9380`

### Step 3: 초기 설정 (웹 UI)

1. 브라우저에서 `http://<서버IP>:9380` 접속
2. 계정 생성
3. **Model Providers 설정:**
   - 임베딩: `bge-m3` (내장, 별도 설정 불필요)
   - Chat LLM: Anthropic 추가 → API 키 입력

### Step 4: 지식베이스(Knowledge Base) 생성

1. Knowledge Base 메뉴 → Create
2. 임베딩 모델: `bge-m3` 선택
3. 파서 설정:
   - 일반 문서: **General** 파서
   - 표가 많은 문서: **Table** 파서
   - Q&A 형식: **Q&A** 파서

### Step 5: 문서 업로드

- 지원 형식: PDF, DOCX, TXT, Markdown, Excel, PPT, 이미지 등
- 업로드 후 자동으로 파싱 → 청킹 → 임베딩 진행
- 진행 상태를 UI에서 확인 가능

### Step 6: 대화(Chat) 설정

1. Chat 메뉴 → Create Assistant
2. 연결할 Knowledge Base 선택
3. LLM: Claude 선택
4. 프롬프트 커스터마이징 (한국어로 답변하도록 시스템 프롬프트 설정)

### Step 7: 테스트

- 질문을 날려보고 검색된 청크와 답변 품질 확인
- 청크가 잘 안 잡히면 청크 크기/파서 설정 조정

---

## 한국어 최적화 팁

### 청킹 설정

- 한국어 텍스트는 청크 크기 **300~500 토큰** 정도가 적당
- 너무 작으면 문맥이 잘림, 너무 크면 검색 정밀도 저하
- PDF보다 **텍스트/마크다운**이 파싱 품질 훨씬 좋음 → 가능하면 md로 변환

### 시스템 프롬프트 예시

```
당신은 사내 문서를 기반으로 질문에 답변하는 어시스턴트입니다.
- 반드시 제공된 문서 내용만을 기반으로 답변하세요.
- 문서에 없는 내용은 "해당 내용을 찾을 수 없습니다"라고 답변하세요.
- 답변은 한국어로 작성하세요.
- 가능하면 출처(문서명, 섹션)를 함께 제시하세요.
```

### 검색 품질 향상

- **하이브리드 검색** 활성화 (벡터 + 키워드) — RAGFlow 기본 지원
- **Reranking** 활성화 — 검색 결과를 한 번 더 정렬해서 정확도 향상
- 문서 업로드 전 불필요한 머리글/바닥글/목차 제거하면 품질 향상

---

## 참고 링크

- RAGFlow GitHub: https://github.com/infiniflow/ragflow
- RAGFlow 문서: https://ragflow.io/docs
- bge-m3 모델: https://huggingface.co/BAAI/bge-m3
